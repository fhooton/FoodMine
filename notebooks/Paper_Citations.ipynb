{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook created by Forrest Hooton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import mfp\n",
    "from src.data_loader import load_ctd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Microsoft Acedemic Graph\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Need to insert api key for microsoft acedimic Graph\n",
    "msft_apikey = 'Insert API key here as string'\n",
    "\n",
    "def get_title(ID):\n",
    "    # Mode can be 'calchistogram', 'evaluate', 'interpret', or 'similarity'\n",
    "    mode = 'evaluate'\n",
    "\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Ocp-Apim-Subscription-Key': msft_apikey,\n",
    "    }\n",
    "    \n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'expr': f\"Id={ID}\",\n",
    "        'count': '10',\n",
    "        'model': 'latest',\n",
    "        'attributes': 'Ti',\n",
    "    })\n",
    "\n",
    "    loaded_eval = query_API(mode, params, headers)\n",
    "    \n",
    "    try:\n",
    "        title = loaded_eval['entities'][0]['Ti']\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    return title\n",
    "\n",
    "\n",
    "def get_citations(paper):\n",
    "    # Code snippits at: https://dev.labs.cognitive.microsoft.com/docs/services/56332331778daf02acc0a50b/operations/56332331778daf06340c9666\n",
    "\n",
    "    # Mode can be 'calchistogram', 'evaluate', 'interpret', or 'similarity'\n",
    "    mode = 'interpret'\n",
    "\n",
    "    query = paper\n",
    "\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Ocp-Apim-Subscription-Key': msft_apikey,\n",
    "    }\n",
    "\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'query': query,\n",
    "        'count': '10',\n",
    "        'model': 'latest',\n",
    "        'attributes': 'Ti',\n",
    "    })\n",
    "\n",
    "    loaded_eval = query_API(mode, params, headers)\n",
    "\n",
    "    # If there are issiues with retrieving info, like no interpretations returned, return 0\n",
    "    try:\n",
    "        paper_query = loaded_interpret['interpretations'][0]['rules'][0]['output']['value']\n",
    "        ID, citations = __get_msft_attribute__(paper_query, 'RId', msft_apikey)\n",
    "    except:\n",
    "        return np.nan, []\n",
    "\n",
    "    return ID, citations\n",
    "\n",
    "# Gets a attribute from paper of microsoft acedemic graph\n",
    "def __get_msft_attribute__(query, attr, key=msft_apikey):\n",
    "    # Mode can be 'calchistogram', 'evaluate', 'interpret', or 'similarity'\n",
    "    mode = 'evaluate'\n",
    "\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Ocp-Apim-Subscription-Key': key,\n",
    "    }\n",
    "\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'expr': query,\n",
    "        'count': '10',\n",
    "        'model': 'latest',\n",
    "        'attributes': attr,\n",
    "    })\n",
    "\n",
    "    loaded_eval = query_API(mode, params, headers)\n",
    "\n",
    "    # See for all attributes: https://docs.microsoft.com/en-us/azure/cognitive-services/academic-knowledge/paperentityattributes\n",
    "    ID = loaded_eval['entities'][0]['Id']\n",
    "    citations = loaded_eval['entities'][0][attr]\n",
    "\n",
    "    return ID, citations\n",
    "\n",
    "def query_API(mode, params, headers):\n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('api.labs.cognitive.microsoft.com')\n",
    "        conn.request(\"GET\", \"/academic/v1.0/\" + mode + \"?%s\" % params, \"{body}\", headers)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read()\n",
    "\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "        \n",
    "    return json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds citations and paper ID to dataframe\n",
    "# Target is the column that holds the paper titles\n",
    "def add_citations(df, target):\n",
    "    papers = pd.DataFrame({ target : df[df[target].notnull()][target].drop_duplicates().tolist() })\n",
    "    \n",
    "    citations_list = []\n",
    "    for idx, row in papers.iterrows():\n",
    "        ID, citations = get_citations(row[target])\n",
    "        \n",
    "        # Float means that the ID is na, aka it did not recognize a paper\n",
    "        if isinstance(ID, float):\n",
    "            citations_list.append(np.nan)\n",
    "            continue\n",
    "        else:\n",
    "            papers.at[idx, 'paper_id'] = ID\n",
    "            citations_list.append(citations)\n",
    "    \n",
    "    papers['citations'] = citations_list\n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load FoodMine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_food_data = pd.read_pickle(mfp('misc_save/garlic_food_data.pkl'))\n",
    "g_food_info = pd.read_csv(mfp('data/garlic_scoring.csv'), encoding='latin1')\n",
    "\n",
    "c_food_data = pd.read_pickle(mfp('misc_save/cocoa_food_data.pkl'))\n",
    "c_food_info = pd.read_csv(mfp('data/cocoa_scoring.csv', encoding='latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all unique PMIDs and store into df\n",
    "g_PMIDs = pd.DataFrame({'PMID' : g_food_data.PMID.drop_duplicates().tolist()}).merge(g_food_info, how = 'left', on = 'PMID')[['PMID', 'paper']]\n",
    "c_PMIDs = pd.DataFrame({'PMID' : c_food_data.PMID.drop_duplicates().tolist()}).merge(c_food_info, how = 'left', on = 'PMID')[['PMID', 'paper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve citation ids from microsoft acedemic graph\n",
    "g_papers = add_citations(g_PMIDs, 'paper')\n",
    "c_papers = add_citations(c_PMIDs, 'paper')\n",
    "\n",
    "#g_papers.to_pickle(mfp('misc_save/garlic_msft.pkl'))\n",
    "#c_papers.to_pickle(mfp('misc_save/cocoa_msft.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_citations = pd.read_pickle(mfp(f'misc_save/garlic_msft.pkl'))\n",
    "c_citations = pd.read_pickle(mfp(f'misc_save/cocoa_msft.pkl'))\n",
    "\n",
    "g_citation_ids = list(set([i for j in g_citations.citations.dropna().tolist() for i in j]))\n",
    "c_citation_ids = list(set([i for j in c_citations.citations.dropna().tolist() for i in j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Retrieve paper titles from MAG\n",
    "titles = []\n",
    "c = 0\n",
    "for p in g_citation_ids:\n",
    "    titles.append(get_title(p))\n",
    "    if not c % 3:\n",
    "        time.sleep(3)\n",
    "        \n",
    "    if not c % 50:\n",
    "        print(f'{c} at {(time.time()-start)/60} min')\n",
    "    c+=1\n",
    "\n",
    "#pd.DataFrame({'id' : g_citation_ids, 'title' : titles}).to_pickle(mfp('misc_save/garlic_citation_titles.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 at 0.055836995442708336 min\n",
      "50 at 1.2028738657633464 min\n",
      "100 at 2.3527114470799764 min\n",
      "150 at 3.50199187596639 min\n",
      "200 at 4.597352143128713 min\n",
      "250 at 5.795228282610576 min\n",
      "300 at 6.960369209448497 min\n",
      "350 at 8.074555778503418 min\n",
      "400 at 9.23499865134557 min\n",
      "450 at 10.397917222976684 min\n",
      "500 at 11.508652718861898 min\n",
      "550 at 12.667752373218537 min\n",
      "600 at 13.829023762543995 min\n",
      "650 at 14.948043950398763 min\n",
      "700 at 16.120840458075204 min\n",
      "750 at 17.280420589447022 min\n",
      "800 at 18.44827857812246 min\n",
      "850 at 19.62849095662435 min\n",
      "900 at 20.791995211442313 min\n",
      "950 at 21.903584138552347 min\n",
      "1000 at 23.066848842302957 min\n",
      "1050 at 24.29008613030116 min\n",
      "1100 at 25.61460832754771 min\n",
      "1150 at 26.776571385065715 min\n",
      "1200 at 27.93707577387492 min\n",
      "1250 at 29.04781185388565 min\n",
      "1300 at 30.221344435214995 min\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Retrieve paper titles from MAG\n",
    "titles = []\n",
    "c = 0\n",
    "for p in c_citation_ids:\n",
    "    titles.append(get_title(p))\n",
    "    if not c % 3:\n",
    "        time.sleep(3)\n",
    "        \n",
    "    if not c % 50:\n",
    "        print(f'{c} at {(time.time()-start)/60} min')\n",
    "    c+=1\n",
    "\n",
    "#pd.DataFrame({'id' : c_citation_ids, 'title' : titles}).to_pickle(mfp('misc_save/cocoa_citation_titles.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ct = pd.read_pickle(mfp('misc_save/garlic_citation_titles.pkl'))\n",
    "c_ct = pd.read_pickle(mfp('misc_save/cocoa_citation_titles.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions are generally the same as misc/pubmed_utils.py, but slightly tweaked\n",
    "\n",
    "import urllib.request as request\n",
    "from lxml import etree\n",
    "import math\n",
    "\n",
    "# Constructs appropriate url for pubmed api from search terms\n",
    "def __construct_url__(url_input, query_type, num_results = 1000000):\n",
    "\n",
    "    # Constructs url for search query\n",
    "    if query_type == 'search':\n",
    "        base_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term='\n",
    "        \n",
    "        if isinstance(url_input, str):\n",
    "            term_url = url_input.replace(\" \", \"%20\")\n",
    "        else:\n",
    "            adjusted_terms = [s.replace(\" \", \"%20\") for s in url_input]\n",
    "            term_url = '%20AND%20'.join(adjusted_terms)\n",
    "\n",
    "        url = base_url + term_url\n",
    "        return url\n",
    "\n",
    "    elif query_type == 'document':\n",
    "        base_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id='\n",
    "\n",
    "        doc_urls = \"\"\n",
    "        for i in url_input:\n",
    "            if isinstance(i, str): \n",
    "                doc_urls = doc_urls + \",\" + i\n",
    "            else:\n",
    "                doc_urls = doc_urls + \",\" + str(i)\n",
    "\n",
    "        url = base_url + doc_urls.lstrip(\",\") + '&retmode=xml'\n",
    "\n",
    "        return url\n",
    "\n",
    "\n",
    "# Divides doc ids for larger paper queries in retrieve_doc_info()\n",
    "def __divide_list__(ids, num_divisions):\n",
    "\n",
    "    split_ids = np.array_split(np.asarray(ids), num_divisions)\n",
    "    split_ids = [np.ndarray.tolist(split_ids[i]) for i in range(len(split_ids))]\n",
    "\n",
    "    return split_ids\n",
    "\n",
    "# Enters search terms into pubmed database to return document ID's\n",
    "def search_pubmed(search_term):\n",
    "\n",
    "    url = __construct_url__(search_term, 'search')\n",
    "    \n",
    "    with request.urlopen(url) as response:\n",
    "        xml = response.read()\n",
    "\n",
    "    root = etree.fromstring(xml)\n",
    "    #print(etree.tostring(root, pretty_print=True))\n",
    "\n",
    "    # Recursively gets all objects where the tag is Id\n",
    "    if (root.findall('.//Count') is None) | (root.findall('.//Count')[0].text == '0'):\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    elements = root.findall('.//Count')\n",
    "    \n",
    "    ID = root.findall('.//Id')[0].text\n",
    "    \n",
    "\n",
    "    # Converts all lxml objects to their text values\n",
    "    ids = [i.text for i in elements]\n",
    "\n",
    "    return ids[0], ID\n",
    "\n",
    "# Retrieves document (paper) info using pubmed paper ids\n",
    "def retrieve_doc_info(ids):\n",
    "    # Can't query too much in a single query, so divides larger id lists into seperate queries\n",
    "    num_loops = int(math.ceil(len(ids) / 100))\n",
    "\n",
    "    # Have to split requests larger than 100 documents to keep it within url size\n",
    "    ids = __divide_list__(ids, num_loops)\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    # Retrieves xml data from pubmed\n",
    "    for i in ids:\n",
    "        url = self.__construct_url__(i, 'document')\n",
    "\n",
    "        with request.urlopen(url) as response:\n",
    "            xml = response.read()\n",
    "\n",
    "        root = etree.fromstring(xml)\n",
    "\n",
    "        documents = documents + root.findall('PubmedArticle')\n",
    "\n",
    "    info = pd.DataFrame()\n",
    "\n",
    "    for document in documents:\n",
    "\n",
    "        doc_id = int(document.find('.//PMID').text)\n",
    "\n",
    "        paper = document.find('.//ArticleTitle').text\n",
    "\n",
    "        journal = document.find('.//Title').text\n",
    "\n",
    "        year = document.find('.//Year').text\n",
    "\n",
    "        if document.find('.//AbstractText') is not None:\n",
    "            abstract = document.find('.//AbstractText').text\n",
    "        else:\n",
    "            abstract = None\n",
    "\n",
    "        mesh_terms = []\n",
    "        mesh_UIds = []\n",
    "        qual_terms = []\n",
    "        qual_UIds = []\n",
    "\n",
    "        for mesh_section in document.findall('.//MeshHeading'):\n",
    "            mesh_terms.append(mesh_section.find('.//DescriptorName').text)\n",
    "            mesh_UIds.append(mesh_section.find('.//DescriptorName').attrib['UI'])\n",
    "\n",
    "            if mesh_section.find('.//QualifierName') is not None:\n",
    "                qual_terms.append(mesh_section.find('.//QualifierName').text)\n",
    "                qual_UIds.append(mesh_section.find('.//QualifierName').attrib['UI'])\n",
    "            else:\n",
    "                qual_terms.append(None)\n",
    "                qual_UIds.append(None)\n",
    "\n",
    "        new_row = {\n",
    "            'PMID' : doc_id,\n",
    "            'paper' : paper,\n",
    "            'journal' : journal,\n",
    "            'year' : year,\n",
    "            'abstract' : abstract,\n",
    "            'mesh_terms' : mesh_terms,\n",
    "            'mesh_UIds' : mesh_UIds,\n",
    "            'qual_terms' : qual_terms,\n",
    "            'qual_UIds' : qual_UIds,\n",
    "            'webpage' : 'https://www.ncbi.nlm.nih.gov/pubmed/' + str(doc_id)\n",
    "        }\n",
    "\n",
    "        info = info.append(new_row, ignore_index = True)\n",
    "\n",
    "    info['PMID'] = info['PMID'].astype('int32')\n",
    "\n",
    "    return info.reset_index(drop = True)\n",
    "\n",
    "def greek_letter_converter(chem):\n",
    "    chem = chem.replace('α ', 'alpha-')\n",
    "    chem = chem.replace('β ', 'beta-')\n",
    "    chem = chem.replace('γ ', 'gamma-')\n",
    "    chem = chem.replace('ρ ', 'rho-')\n",
    "    chem = chem.replace('δ ', 'delta-')\n",
    "\n",
    "    return chem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ct.title = g_ct.title.apply(greek_letter_converter)\n",
    "\n",
    "# Query pubmed with titles to find PMID\n",
    "start = time.time()\n",
    "c=0\n",
    "for idx, row in g_ct.iterrows():\n",
    "    try:\n",
    "        _, ID = search_pubmed(row['title'])\n",
    "        g_ct.at[idx, 'pubmed_id'] = ID\n",
    "        time.sleep(.5)\n",
    "    except:\n",
    "        g_ct.at[idx, 'pubmed_id'] = 0\n",
    "        pass\n",
    "    \n",
    "    if not c % 50:\n",
    "        print(f'{c} at {(time.time()-start)/60} min')\n",
    "    c+=1\n",
    "    \n",
    "#g_ct.to_pickle(mfp('misc_save/garlic_citation_PMIDs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ct.title = c_ct.title.apply(greek_letter_converter)\n",
    "\n",
    "# Query pubmed with titles to find PMID\n",
    "start = time.time()\n",
    "c=0\n",
    "for idx, row in c_ct.iterrows():\n",
    "    try:\n",
    "        _, ID = search_pubmed(row['title'])\n",
    "        g_ct.at[idx, 'pubmed_id'] = ID\n",
    "        time.sleep(.5)\n",
    "    except:\n",
    "        g_ct.at[idx, 'pubmed_id'] = 0\n",
    "        pass\n",
    "    \n",
    "    if not c % 50:\n",
    "        print(f'{c} at {(time.time()-start)/60} min')\n",
    "    c+=1\n",
    "    \n",
    "#c_ct.to_pickle(mfp('misc_save/cocoa_citation_PMIDs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ct = pd.read_pickle(mfp('misc_save/garlic_citation_PMIDs.pkl'))\n",
    "c_ct = pd.read_pickle(mfp('misc_save/cocoa_citation_PMIDs.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in CTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\forresthooton\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "hdata = load_ctd()\n",
    "\n",
    "de_health = pd.DataFrame(\n",
    "    hdata[hdata.pubchem_id.notnull() & hdata.PubMedIDs.notnull()][['pubchem_id', 'PubMedIDs', 'ChemicalName']]\n",
    "    .groupby(['pubchem_id','PubMedIDs']).count()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65016"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_health.PubMedIDs.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_fm = pd.read_pickle(mfp('misc_save/garlic_food_data.pkl'))\n",
    "c_fm = pd.read_pickle(mfp('misc_save/cocoa_food_data.pkl'))\n",
    "\n",
    "# Use papers for CTD compounds that appear in the food pilots\n",
    "g_fm = g_fm.merge(de_health, how='inner', on = 'pubchem_id')\n",
    "c_fm = c_fm.merge(de_health, how='inner', on = 'pubchem_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_papers_g = pd.DataFrame({'PMID' : list(set([int(i) for j in g_fm.PubMedIDs.dropna().str.split('|').tolist() for i in j]))})\n",
    "h_papers_c = pd.DataFrame({'PMID' : list(set([int(i) for j in c_fm.PubMedIDs.dropna().str.split('|').tolist() for i in j]))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Paper Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PMID, index]\n",
       "Index: []"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = h_papers_g.merge(pd.DataFrame(g_food_data.PMID.drop_duplicates().reset_index()), how='inner')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PMID, index]\n",
       "Index: []"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = h_papers_c.merge(pd.DataFrame(c_food_data.PMID.drop_duplicates().reset_index()), how='inner')\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23430952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PMID\n",
       "0  23430952"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_list = g_ct[g_ct.pubmed_id.notnull()].pubmed_id.apply(int).tolist()\n",
    "\n",
    "g_cda = pd.DataFrame({'PMID' : g_list})\n",
    "\n",
    "g_overlap = h_papers_g.merge(g_cda, how='inner')\n",
    "g_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30336258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PMID\n",
       "0  30336258"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_list = c_ct[c_ct.pubmed_id.notnull()].pubmed_id.apply(int).tolist()\n",
    "\n",
    "c_cda = pd.DataFrame({'PMID' : c_list})\n",
    "\n",
    "c_overlap = h_papers_c.merge(c_cda, how='inner')\n",
    "c_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
